{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "from alibabacloud_sls20201230.client import Client as Sls20201230Client\n",
    "from alibabacloud_tea_openapi import models as open_api_models\n",
    "from alibabacloud_sls20201230 import models as sls_20201230_models\n",
    "from alibabacloud_tea_util import models as util_models\n",
    "from alibabacloud_tea_util.client import Client as UtilClient\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "ALIBABA_CLOUD_ACCESS_KEY_ID = os.environ[\"ALIBABA_CLOUD_ACCESS_KEY_ID\"]\n",
    "ALIBABA_CLOUD_ACCESS_KEY_SECRET = os.environ[\"ALIBABA_CLOUD_ACCESS_KEY_SECRET\"]\n",
    "\n",
    "HOURS_TO_RUN = int(os.getenv(\"HOURS_TO_RUN\", \"8\"))\n",
    "MINUTES_INTERVAL = int(os.getenv(\"MINUTES_INTERVAL\", \"240\"))\n",
    "MINUTES_INTERVAL_SMALL = int(os.getenv(\"MINUTES_INTERVAL_SMALL\", \"15\"))\n",
    "ROUNDS_TO_RUN = int(os.getenv(\"ROUNDS_TO_RUN\", \"19\"))\n",
    "\n",
    "RUN_CONTINUOUSLY = \"true\" == os.getenv(\"RUN_CONTINUOUSLY\", \"false\")\n",
    "\n",
    "MYSQL_HOST = os.getenv(\"MYSQL_HOST\", \"0.0.0.0\")\n",
    "MYSQL_PORT = int(os.getenv(\"MYSQL_PORT\", \"3306\"))\n",
    "MYSQL_USER_NAME = os.getenv(\"MYSQL_USER_NAME\", \"root\")\n",
    "MYSQL_PASSWORD = os.getenv(\"MYSQL_PASSWORD\", \"peng_mbp13\")\n",
    "\n",
    "config = open_api_models.Config(\n",
    "    access_key_id=ALIBABA_CLOUD_ACCESS_KEY_ID,\n",
    "    access_key_secret=ALIBABA_CLOUD_ACCESS_KEY_SECRET,\n",
    "    read_timeout=120 * 1000,\n",
    "    connect_timeout=10 * 1000,\n",
    "    no_proxy=\"cn-hangzhou.log.aliyuncs.com\",\n",
    ")\n",
    "# Endpoint 请参考 https://api.aliyun.com/product/Sls\n",
    "config.endpoint = f\"cn-hangzhou.log.aliyuncs.com\"\n",
    "sls_client = Sls20201230Client(config)\n",
    "sls_client._read_timeout = 120 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import threading\n",
    "\n",
    "\n",
    "def write_data_with_sql(sql) -> int:\n",
    "    # Create a connection to the MySQL server\n",
    "    conn = pymysql.connect(\n",
    "        host=MYSQL_HOST,\n",
    "        port=MYSQL_PORT,\n",
    "        user=MYSQL_USER_NAME,\n",
    "        password=MYSQL_PASSWORD,\n",
    "        database=\"grafana\",\n",
    "    )\n",
    "    rows_inserted = 0\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        sql = sql.replace(\"'null'\", \"null\")\n",
    "        rows_inserted = cursor.execute(sql)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"sql:{sql}\\nError inserting data: {e}\")\n",
    "        conn.rollback()\n",
    "        raise e\n",
    "    else:\n",
    "        conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return rows_inserted\n",
    "\n",
    "\n",
    "WRITE_MYSQL_ROWS_BUFF = 500\n",
    "\n",
    "# Dictionary to store locks for each resource\n",
    "resource_locks = {}\n",
    "lock_dict_lock = threading.Lock()\n",
    "\n",
    "def get_lock_for_resource(resource_name):\n",
    "    with lock_dict_lock:  # Ensure thread-safe access to the dictionary\n",
    "        if resource_name not in resource_locks:\n",
    "            resource_locks[resource_name] = threading.Lock()\n",
    "        return resource_locks[resource_name]\n",
    "\n",
    "def write_df_to_mysql(\n",
    "    df: pd.DataFrame,\n",
    "    table_name=\"product_views\",\n",
    "    columns=[\n",
    "        \"minute_of_day\",\n",
    "        \"day\",\n",
    "        \"time_of_day\",\n",
    "        \"pdid\",\n",
    "        \"sku\",\n",
    "        \"name\",\n",
    "        \"views\",\n",
    "        \"avg_sale_price\",\n",
    "        \"max_sale_price\",\n",
    "        \"min_sale_price\",\n",
    "        \"viewed_users\",\n",
    "    ],\n",
    "    colomns_to_update=[\n",
    "        \"views\",\n",
    "        \"avg_sale_price\",\n",
    "        \"max_sale_price\",\n",
    "        \"min_sale_price\",\n",
    "        \"viewed_users\",\n",
    "    ],\n",
    ") -> int:\n",
    "    if df is None or columns is None or len(columns) <= 0:\n",
    "        logging.warning(\"这是一个空数据集...无须插入\")\n",
    "        return 0\n",
    "    \n",
    "    # Get the lock for the specific resource\n",
    "    resource_lock = get_lock_for_resource(table_name)\n",
    "    \n",
    "    # Use the lock in a context manager\n",
    "    with resource_lock:\n",
    "        df = df[columns]\n",
    "        update_colomns = [f\"{col}=values({col})\" for col in colomns_to_update]\n",
    "        update_colomns = \",\".join(update_colomns)\n",
    "        update_colomns = f\"{update_colomns},update_time=now()\"\n",
    "        sql_template = f\"\"\"INSERT INTO {table_name}({','.join(columns)}) \\nvalues __VALUES__ ON DUPLICATE KEY UPDATE \n",
    "        {update_colomns};\"\"\"\n",
    "        values_list = []\n",
    "        inserted_rows = 0\n",
    "        for index, row in df.iterrows():\n",
    "            value_of_row = []\n",
    "            for col in columns:\n",
    "                col_value_without_quote = f\"{row[col]}\".replace(\"'\", \"\\\\'\")\n",
    "                value_of_row.append(f\"'{col_value_without_quote}'\")\n",
    "            values_list.append(f\"\"\"({','.join(value_of_row)})\"\"\")\n",
    "            if len(values_list) >= WRITE_MYSQL_ROWS_BUFF:\n",
    "                logging.info(f\"about to write batch of data: {len(values_list)}\")\n",
    "                sql = sql_template.replace(\"__VALUES__\", \",\\n\".join(values_list))\n",
    "                inserted_rows = inserted_rows + write_data_with_sql(sql=sql)\n",
    "                logging.info(f\"write_data_with_sql() total afected so far:{inserted_rows}\")\n",
    "                values_list = []\n",
    "        if len(values_list) > 0:\n",
    "            logging.info(f\"about to write last batch of data:{len(values_list)}\")\n",
    "            sql = sql_template.replace(\"__VALUES__\", \",\\n\".join(values_list))\n",
    "            if len(values_list) <= 10:\n",
    "                logging.info(f\"SQL监控:{sql}\")\n",
    "            inserted_rows = inserted_rows + write_data_with_sql(sql=sql)\n",
    "            logging.info(f\"write_data_with_sql() total afected so far:{inserted_rows}\")\n",
    "            values_list = []\n",
    "\n",
    "        logging.info(f\"写入了:{inserted_rows}条数据到MySQL\")\n",
    "        return inserted_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取SLS日志的查询结果\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"user-agent\": \"AlibabaCloud API Workbench\",\n",
    "    \"x-log-apiversion\": \"0.6.0\",\n",
    "    \"x-log-bodyrawsize\": \"0\",\n",
    "    \"x-log-signaturemethod\": \"hmac-sha256\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_sls_data_by_query(\n",
    "    from_time: datetime,\n",
    "    to_time: datetime,\n",
    "    query=\"\",\n",
    "    project=\"xianmu-front-end-log\",\n",
    "    logstore=\"xm-mall\",\n",
    "    retry_time=1,\n",
    ") -> pd.DataFrame:\n",
    "    if retry_time < 0:\n",
    "        logging.error(f\"超过了最多重试次数\")\n",
    "        return None\n",
    "    logging.info(\n",
    "        f\"即将获取数据: =====>from_time:{from_time}, to_time:{to_time}, logstore:{logstore}, query:{query[0:150]}\",\n",
    "    )\n",
    "\n",
    "    from_ = int(from_time.timestamp())\n",
    "    to_ = int(to_time.timestamp())\n",
    "\n",
    "    get_logs_v2headers = sls_20201230_models.GetLogsV2Headers(\n",
    "        common_headers=headers, accept_encoding=\"lz4\"\n",
    "    )\n",
    "    get_logs_v2request = sls_20201230_models.GetLogsV2Request(\n",
    "        from_=from_,\n",
    "        to=to_,\n",
    "        query=query,\n",
    "    )\n",
    "    runtime = util_models.RuntimeOptions(\n",
    "        connect_timeout=10 * 1000,\n",
    "        read_timeout=120 * 1000,\n",
    "        no_proxy=\"cn-hangzhou.log.aliyuncs.com\",\n",
    "        max_attempts=2,\n",
    "    )\n",
    "\n",
    "    product_view_data = []\n",
    "    try:\n",
    "        response = sls_client.get_logs_v2with_options(\n",
    "            project=project,\n",
    "            logstore=logstore,\n",
    "            request=get_logs_v2request,\n",
    "            headers=get_logs_v2headers,\n",
    "            runtime=runtime,\n",
    "        )\n",
    "        product_view_data = response.body.data\n",
    "        logging.info(f\">=====数据条数:{len(product_view_data)}\")\n",
    "        return pd.DataFrame(product_view_data)\n",
    "    except Exception as error:\n",
    "        logging.error(\n",
    "            f\"查询SLS失败了,重试:{retry_time},project:{project},logstore:{logstore}, 错误:{error}\"\n",
    "        )\n",
    "        # 5 秒后重试一次\n",
    "        time.sleep(5)\n",
    "        return get_sls_data_by_query(\n",
    "            from_time=from_time,\n",
    "            to_time=to_time,\n",
    "            query=query,\n",
    "            project=project,\n",
    "            logstore=logstore,\n",
    "            retry_time=retry_time - 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取SLS日志的查询结果\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"user-agent\": \"AlibabaCloud API Workbench\",\n",
    "    \"x-log-apiversion\": \"0.6.0\",\n",
    "    \"x-log-bodyrawsize\": \"0\",\n",
    "    \"x-log-signaturemethod\": \"hmac-sha256\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_product_view_data_and_save(from_time: datetime, to_time: datetime):\n",
    "    product_view_data_df = None\n",
    "    query = \"\"\"\n",
    "    type:view|select * from (select *,row_number() over( partition by minute_of_day order by views desc ) as rnk \n",
    "    from (select minute_of_day,split(minute_of_day,' ')[1] as day,split(minute_of_day,' ')[2] as time_of_day,\n",
    "        pdid,sku,name, count(0)views,round(avg(salePrice),2)avg_sale_price,max(salePrice) max_sale_price,\n",
    "        min(salePrice) min_sale_price, count(distinct uid) as viewed_users \n",
    "        from (select uid,date_format(__time__,'%Y-%m-%d %H:%i:00') minute_of_day,pageName, \n",
    "            replace(regexp_extract(product_viewed, 'name:(?<name>[^,]+)'), 'name:','') as name, \n",
    "            replace(regexp_extract(product_viewed, 'sku:(?<sku>[^,]+)'), 'sku:','') as sku, \n",
    "            replace(regexp_extract(product_viewed, 'pdid:(?<pdid>[^,]+)'), 'pdid:','') as pdid, \n",
    "            cast(replace(regexp_extract(product_viewed, 'salePrice:(?<saleprice>[^,]+)'), 'salePrice:','') as double) as salePrice, \n",
    "            cast(replace(regexp_extract(product_viewed, 'stock:(?<stock>[^,]+)'), 'stock:','') as bigint) as stock \n",
    "            from log, unnest(split(bid,';')) as t(product_viewed) having sku is not null limit 1000000) \n",
    "            group by minute_of_day,pdid,sku,name)) \n",
    "        where rnk<=100 order by minute_of_day\n",
    "    \"\"\"\n",
    "    # 商品的曝光数据只取top 1000的商品\n",
    "\n",
    "    product_view_data_df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=query,\n",
    "    )\n",
    "    if product_view_data_df is None:\n",
    "        logging.error(f\"没有获取到商品的曝光数据:{from_time}~{to_time}\")\n",
    "        return\n",
    "    # logging.info(f\"过滤后的长度：{len(product_view_data_df)}\")\n",
    "    write_df_to_mysql(product_view_data_df)\n",
    "\n",
    "\n",
    "def get_product_views_summary_and_save(from_time: datetime, to_time: datetime):\n",
    "    df = None\n",
    "    query = \"\"\"\n",
    "    type:view|select minute_of_day,case when stock>0 then 'normal' else 'out-of-stock' end as is_out_of_stock,\n",
    "        count(0)views,\n",
    "        count(distinct uid) as viewed_users,\n",
    "        count(distinct sku) as viewed_skus\n",
    "    from (select uid,date_format(__time__,'%Y-%m-%d %H:%i:00') minute_of_day,\n",
    "            replace(regexp_extract(product_viewed, 'sku:(?<sku>[^,]+)'), 'sku:','') as sku,\n",
    "            cast(replace(regexp_extract(product_viewed, 'stock:(?<stock>[^,]+)'), 'stock:','') as bigint) as stock\n",
    "        from log, unnest(split(bid,';')) as t(product_viewed) having sku is not null limit 1000000) \n",
    "    group by minute_of_day,is_out_of_stock order by minute_of_day,is_out_of_stock limit 50000\"\"\"\n",
    "\n",
    "    # 商品曝光数据，分钟级别汇总\n",
    "\n",
    "    df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    if df is None or len(df)<=0:\n",
    "        logging.error(f\"未获取到商品曝光数据:{from_time}, {to_time}\")\n",
    "        return\n",
    "\n",
    "    write_df_to_mysql(\n",
    "        df=df,\n",
    "        table_name=\"product_views_summary\",\n",
    "        columns=[\n",
    "            \"minute_of_day\",\n",
    "            \"is_out_of_stock\",\n",
    "            \"views\",\n",
    "            \"viewed_users\",\n",
    "            \"viewed_skus\",\n",
    "        ],\n",
    "        colomns_to_update=[\n",
    "            \"views\",\n",
    "            \"viewed_users\",\n",
    "            \"viewed_skus\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_backend_query = \"\"\"\n",
    "(inboundFlag:GET or inboundFlag:POST) and \"xm-phone\" not null | \n",
    "select second_of_hour,round(avg(uv)) avg_uv_30s,max(uv) max_uv_30s,round(avg(request_cnt)) avg_qps_30s,\n",
    "    max(request_cnt) max_qps_30s,sum(request_cnt) request_cnt \n",
    "from (select count(distinct \"xm-uid\") as uv,count(distinct concat(\"xm-uid\",\"xm-rqid\")) as request_cnt,\n",
    "    date_format(time, '%Y-%m-%d %H:%i:00') as second_of_hour,date_format(time, '%H:%i:%S') as second_of_hour_real \n",
    "from log group by second_of_hour,second_of_hour_real limit 360000) \n",
    "group by second_of_hour order by second_of_hour\"\"\"\n",
    "\n",
    "\n",
    "def get_mall_qps_data_and_save(from_time: datetime, to_time: datetime):\n",
    "    mall_qps_df = None\n",
    "    mall_qps_df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=mall_backend_query,\n",
    "        project=\"k8s-log-c7d28cba17d0a416ca4f52459592b8d38\",\n",
    "        logstore=\"prod-mall-stdout-log\",\n",
    "    )\n",
    "\n",
    "    if mall_qps_df is None or len(mall_qps_df)<=0:\n",
    "        logging.error(f\"未获取到QPS数据:{from_time}, {to_time}\")\n",
    "        return\n",
    "\n",
    "    write_df_to_mysql(\n",
    "        mall_qps_df,\n",
    "        table_name=\"mall_http_api_qps\",\n",
    "        columns=[\n",
    "            \"second_of_hour\",\n",
    "            \"avg_uv_30s\",\n",
    "            \"max_uv_30s\",\n",
    "            \"avg_qps_30s\",\n",
    "            \"max_qps_30s\",\n",
    "            \"request_cnt\",\n",
    "        ],\n",
    "        colomns_to_update=[\n",
    "            \"avg_uv_30s\",\n",
    "            \"max_uv_30s\",\n",
    "            \"avg_qps_30s\",\n",
    "            \"max_qps_30s\",\n",
    "            \"request_cnt\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_orders_query = \"\"\"\n",
    "ap:https\\://h5.summerfarm.net/order/ |  select minute_of_day,json_extract_scalar(orderItems,'$.sku') sku,\n",
    "    json_extract_scalar(orderItems,'$.pdName') pd_name ,json_extract_scalar(orderItems,'$.itemCategory') as category,\n",
    "    count(distinct uid) ordered_users, sum(cast(json_extract_scalar(orderItems,'$.amount') as bigint)) as total_quantity ,\n",
    "    round(sum(cast(json_extract_scalar(orderItems,'$.actualTotalPrice') as double)),2) as total_gmv \n",
    "from (\n",
    "    select orderItems,date_format(__time__,'%Y-%m-%d %H:%i:00') as minute_of_day,uid \n",
    "        from log, unnest(cast(json_extract(ai, '$.rt.data.orderItems') as array(json))) as t(orderItems)  \n",
    "        having orderItems is not null limit 1000000) \n",
    "group by minute_of_day,sku,pd_name,category order by minute_of_day,total_gmv desc limit 300000\"\"\"\n",
    "\n",
    "\n",
    "def get_product_orders_and_save(from_time: datetime, to_time: datetime):\n",
    "    orders_df = None\n",
    "    orders_df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=sku_orders_query,\n",
    "        project=\"xianmu-front-end-log\",\n",
    "        logstore=\"xm-mall\",\n",
    "    )\n",
    "\n",
    "    if orders_df is None or len(orders_df)<=0:\n",
    "        logging.error(f\"未获取到前端上报到订单数据:{from_time}, {to_time}\")\n",
    "        return\n",
    "\n",
    "    write_df_to_mysql(\n",
    "        orders_df,\n",
    "        table_name=\"product_orders\",\n",
    "        columns=[\n",
    "            \"minute_of_day\",\n",
    "            \"sku\",\n",
    "            \"pd_name\",\n",
    "            \"category\",\n",
    "            \"ordered_users\",\n",
    "            \"total_quantity\",\n",
    "            \"total_gmv\",\n",
    "        ],\n",
    "        colomns_to_update=[\"ordered_users\", \"total_quantity\", \"total_gmv\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from concurrent import futures\n",
    "import concurrent\n",
    "import time\n",
    "\n",
    "\n",
    "def process_timerange(start_time_of_run, end_time_of_run) -> bool:\n",
    "    try:\n",
    "        logging.info(\"获取分钟级别汇总的商品曝光数\")\n",
    "        get_product_views_summary_and_save(start_time_of_run, end_time_of_run)\n",
    "\n",
    "        logging.info(\"获取商品曝光数据:\")\n",
    "        get_product_view_data_and_save(start_time_of_run, end_time_of_run)\n",
    "\n",
    "        logging.info(\"获取后端QPS数据\")\n",
    "        get_mall_qps_data_and_save(start_time_of_run, end_time_of_run)\n",
    "\n",
    "        logging.info(\"获取商品下单数据：前端SLS日志\")\n",
    "        get_product_orders_and_save(start_time_of_run, end_time_of_run)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"发生了异常, start_time_of_run:{start_time_of_run}, end_time_of_run:{end_time_of_run}, error:{e}\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "\n",
    "running_counter = 1\n",
    "INIT_LAST_7DAYS_DATA = os.getenv(\"INIT_LAST_7DAYS_DATA\", \"false\")\n",
    "INIT_LAST_7DAYS_DATA = f\"{INIT_LAST_7DAYS_DATA}\" == \"true\"\n",
    "PYTHON_RUN_INTERVAL = int(os.getenv(\"PYTHON_RUN_INTERVAL\", \"45\"))\n",
    "INIT_LAST_N_DAYS = int(os.getenv(\"INIT_LAST_N_DAYS\", \"3\"))\n",
    "while running_counter > 0:\n",
    "    # 对时间的秒数取整，比如 2024-01-01 00:01:00\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "    now = datetime.strptime(now, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time_of_run = now\n",
    "    if INIT_LAST_7DAYS_DATA:\n",
    "        MINUTES_INTERVAL_SMALL = MINUTES_INTERVAL\n",
    "        ROUNDS_TO_RUN = int(24 * 60 / MINUTES_INTERVAL * INIT_LAST_N_DAYS + 1)\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_list=[]\n",
    "        for interval in range(1, ROUNDS_TO_RUN):\n",
    "            start_time_of_run = end_time_of_run - timedelta(minutes=MINUTES_INTERVAL_SMALL)\n",
    "            future_list.append(executor.submit(process_timerange, start_time_of_run, end_time_of_run))\n",
    "            end_time_of_run = start_time_of_run\n",
    "        \n",
    "        concurrent.futures.wait(future_list)\n",
    "        \n",
    "    if \"false\" == f\"{RUN_CONTINUOUSLY}\".lower():\n",
    "        logging.info(f\"系统没有设置持续性的跑,RUN_CONTINUOUSLY:{RUN_CONTINUOUSLY}\")\n",
    "        break\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"是否持续性的跑:{RUN_CONTINUOUSLY},running_counter:{running_counter}, PYTHON_RUN_INTERVAL:{PYTHON_RUN_INTERVAL}s\"\n",
    "        )\n",
    "        running_counter = running_counter + 1\n",
    "        time.sleep(PYTHON_RUN_INTERVAL)\n",
    "        INIT_LAST_7DAYS_DATA = False\n",
    "        MINUTES_INTERVAL_SMALL = 30\n",
    "        ROUNDS_TO_RUN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
