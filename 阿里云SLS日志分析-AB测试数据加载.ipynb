{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "from alibabacloud_sls20201230.client import Client as Sls20201230Client\n",
    "from alibabacloud_tea_openapi import models as open_api_models\n",
    "from alibabacloud_sls20201230 import models as sls_20201230_models\n",
    "from alibabacloud_tea_util import models as util_models\n",
    "from alibabacloud_tea_util.client import Client as UtilClient\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import logging\n",
    "\n",
    "ds_to_run=datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "ALIBABA_CLOUD_ACCESS_KEY_ID = os.environ[\"ALIBABA_CLOUD_ACCESS_KEY_ID\"]\n",
    "ALIBABA_CLOUD_ACCESS_KEY_SECRET = os.environ[\"ALIBABA_CLOUD_ACCESS_KEY_SECRET\"]\n",
    "\n",
    "config = open_api_models.Config(\n",
    "    access_key_id=ALIBABA_CLOUD_ACCESS_KEY_ID,\n",
    "    access_key_secret=ALIBABA_CLOUD_ACCESS_KEY_SECRET,\n",
    "    read_timeout=120 * 1000,\n",
    "    connect_timeout=10 * 1000,\n",
    "    no_proxy=\"cn-hangzhou.log.aliyuncs.com\",\n",
    ")\n",
    "# Endpoint 请参考 https://api.aliyun.com/product/Sls\n",
    "config.endpoint = f\"cn-hangzhou.log.aliyuncs.com\"\n",
    "sls_client = Sls20201230Client(config)\n",
    "sls_client._read_timeout = 120 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取SLS日志的查询结果\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"user-agent\": \"AlibabaCloud API Workbench\",\n",
    "    \"x-log-apiversion\": \"0.6.0\",\n",
    "    \"x-log-bodyrawsize\": \"0\",\n",
    "    \"x-log-signaturemethod\": \"hmac-sha256\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_sls_data_by_query(\n",
    "    from_time: datetime,\n",
    "    to_time: datetime,\n",
    "    query: str = \"\",\n",
    "    project: str = \"xianmu-front-end-log\",\n",
    "    logstore: str = \"xm-mall\",\n",
    "    retry_time: int = 1,\n",
    "    line: int = 100,\n",
    "    offset: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    if retry_time < 0:\n",
    "        logging.error(f\"超过了最多重试次数\")\n",
    "        return None\n",
    "    if offset % 10000 == 0:\n",
    "        logging.info(\n",
    "            f\"即将获取数据: =====>from_time:{from_time}, to_time:{to_time}, logstore:{logstore}, query:{query[0:150]}\",\n",
    "        )\n",
    "\n",
    "    from_ = int(from_time.timestamp())\n",
    "    to_ = int(to_time.timestamp())\n",
    "\n",
    "    get_logs_v2headers = sls_20201230_models.GetLogsV2Headers(\n",
    "        common_headers=headers, accept_encoding=\"lz4\"\n",
    "    )\n",
    "    get_logs_v2request = sls_20201230_models.GetLogsV2Request(\n",
    "        from_=from_,\n",
    "        to=to_,\n",
    "        query=query,\n",
    "        line=line,\n",
    "        offset=offset,\n",
    "    )\n",
    "    runtime = util_models.RuntimeOptions(\n",
    "        connect_timeout=10 * 1000,\n",
    "        read_timeout=120 * 1000,\n",
    "        no_proxy=\"cn-hangzhou.log.aliyuncs.com\",\n",
    "        max_attempts=2,\n",
    "    )\n",
    "\n",
    "    product_view_data = []\n",
    "    try:\n",
    "        response = sls_client.get_logs_v2with_options(\n",
    "            project=project,\n",
    "            logstore=logstore,\n",
    "            request=get_logs_v2request,\n",
    "            headers=get_logs_v2headers,\n",
    "            runtime=runtime,\n",
    "        )\n",
    "        product_view_data = response.body.data\n",
    "        if offset % 10000 == 0:\n",
    "            logging.info(f\">=====数据条数:{len(product_view_data)}\")\n",
    "        return pd.DataFrame(product_view_data)\n",
    "    except Exception as error:\n",
    "        logging.error(\n",
    "            f\"查询SLS失败了,重试:{retry_time},project:{project},logstore:{logstore}, 错误:{error}\"\n",
    "        )\n",
    "        # 5 秒后重试一次\n",
    "        time.sleep(5)\n",
    "        return get_sls_data_by_query(\n",
    "            from_time=from_time,\n",
    "            to_time=to_time,\n",
    "            query=query,\n",
    "            project=project,\n",
    "            logstore=logstore,\n",
    "            retry_time=retry_time - 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取SLS日志的查询结果\n",
    "# 获取用户的分桶信息\n",
    "\n",
    "\n",
    "def get_mid_variant_info(from_time: datetime, to_time: datetime):\n",
    "    user_ab_test_info_df = None\n",
    "    query = \"\"\"\n",
    "    ap:/abStrategy/userExperiments | select phone as cust_phone,uid as cust_id,\n",
    "        json_extract_scalar(json_extract_scalar(ai, '$.rt'),'$.data[\"new-home\"].variantId') variant_id,\n",
    "        json_extract_scalar(json_extract_scalar(ai, '$.rt'),'$.data[\"new-home\"].experimentId') experiment_id,\n",
    "        count(1) request_cnt,\n",
    "        date_format(min(__time__),'%Y-%m-%d %H:%i:%s') min_time,\n",
    "        date_format(max(__time__),'%Y-%m-%d %H:%i:%s') max_time\n",
    "    from log \n",
    "    where phone is not null and length(phone)>1 group by 1,2,3,4 \n",
    "    having variant_id is not null limit 1000000\n",
    "    \"\"\"\n",
    "\n",
    "    user_ab_test_info_df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=query,\n",
    "    )\n",
    "    if user_ab_test_info_df is None:\n",
    "        logging.error(f\"没有获取到商品的曝光数据:{from_time}~{to_time}\")\n",
    "        return\n",
    "    \n",
    "    return user_ab_test_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取用户的点击信息（点击包括了商品详情页的点击、加入购物车的点击、以及立即购买的点击）\n",
    "\n",
    "\n",
    "def get_user_click_data(\n",
    "    from_time: datetime = datetime.now() - timedelta(hours=1),\n",
    "    to_time: datetime = datetime.now(),\n",
    "    page_name: str = \"/home\",\n",
    ") -> pd.DataFrame:\n",
    "    query = f\"\"\"type:cl and pageName:{page_name} and uid not null and sku |\n",
    "        select uid,bid,cid,pid,sku,pageName,url,date_format(__time__,'%Y-%m-%d %H:%i:%S') as arrive_time \n",
    "        from log where uid is not null limit 1000000\"\"\"\n",
    "    # user_click_df = pd.DataFrame()\n",
    "    # offset = 0\n",
    "\n",
    "    # while True:\n",
    "    batch_df = get_sls_data_by_query(\n",
    "        from_time=from_time, to_time=to_time, query=query\n",
    "        # , offset=offset\n",
    "    )\n",
    "\n",
    "    if batch_df is None or len(batch_df) == 0:\n",
    "        # if offset == 0:\n",
    "        #     logging.error(f\"没有获取到商品的曝光数据:{from_time}~{to_time}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 删除 'userAgent' 和 'v' 列\n",
    "    # batch_df = batch_df.drop(\n",
    "    #     columns=[\"userAgent\", \"useragent\", \"v\"], errors=\"ignore\"\n",
    "    # )\n",
    "\n",
    "    # 确保 uid 列不为 None、nan 或非数字，并将其转换为 int 类型\n",
    "    batch_df[\"uid\"] = pd.to_numeric(batch_df[\"uid\"], errors=\"coerce\")\n",
    "    batch_df[\"uid\"] = batch_df[\"uid\"].fillna(-1)\n",
    "    batch_df[\"uid\"] = batch_df[\"uid\"].astype(int)\n",
    "    # 删除任何仍然包含非数字的行\n",
    "    batch_df = batch_df[batch_df[\"uid\"] != -1]\n",
    "    return batch_df\n",
    "\n",
    "        # user_click_df = pd.concat([user_click_df, batch_df], ignore_index=True)\n",
    "\n",
    "        # if len(batch_df) < 100:\n",
    "        #     print(\n",
    "        #         f\"from_time:{from_time}, to_time:{to_time}, page_name:{page_name} 已经获取完毕了\"\n",
    "        #     )\n",
    "        #     break\n",
    "\n",
    "        # offset = len(user_click_df)\n",
    "        # if offset % 10000 == 0:\n",
    "        #     print(f\"获取更多数据, 新的offset:{offset}\")\n",
    "\n",
    "    # return user_click_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是获取用户分桶数据并保存\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from odps_client import write_pandas_df_into_odps\n",
    "\n",
    "N = 4  # Number of days to loop over\n",
    "end_date = datetime.now().date()\n",
    "\n",
    "for i in range(N):\n",
    "    current_date = end_date - timedelta(days=i)\n",
    "    from_time = datetime.combine(current_date, datetime.min.time())\n",
    "    to_time = from_time + timedelta(hours=24)\n",
    "    ds = from_time.strftime(\"%Y%m%d\")\n",
    "\n",
    "    print(f\"Processing data for {from_time} to {to_time}, ds:{ds}\")\n",
    "\n",
    "    df = get_mid_variant_info(from_time=from_time, to_time=to_time)\n",
    "    df.drop(columns=[\"__source__\", \"__time__\"], inplace=True)\n",
    "\n",
    "    table_name = \"summerfarm_ds.temp_mall_new_home_ab_info_di\"\n",
    "\n",
    "    partition_spec = f\"ds={ds}\"\n",
    "    is_ok = write_pandas_df_into_odps(\n",
    "        df=df,\n",
    "        table_name=table_name,\n",
    "        partition_spec=partition_spec,\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    print(f\"save {len(df)} records into table:{table_name}, ds:{ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是获取用户点击数据并保存\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from odps_client import write_pandas_df_into_odps\n",
    "\n",
    "\n",
    "def get_csv_location_by_ds(ds: str) -> str:\n",
    "    return f\"./data/user_click_all_new_df_{ds}.csv\"\n",
    "\n",
    "\n",
    "# 如果CSV文件没有数据，则从get_user_click_data获取数据\n",
    "def load_user_click_data(ds_to_run: str) -> pd.DataFrame:\n",
    "    # 尝试从CSV文件读取数据\n",
    "    csv_file_path = get_csv_location_by_ds(ds_to_run)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        if not df.empty:\n",
    "            print(f\"从CSV文件加载了{len(df)}条记录\")\n",
    "            return df\n",
    "\n",
    "    print(f\"未找到日期:{ds_to_run}的数据\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "start_date = datetime(2024, 9, 10).date()\n",
    "end_date = datetime.now().date()\n",
    "N = (end_date - start_date).days\n",
    "print(f\"start_date:{start_date}, end_date:{end_date}, N:{N}\")\n",
    "\n",
    "user_click_all_df = pd.DataFrame()\n",
    "\n",
    "for i in range(N):\n",
    "    from_time = datetime.combine(start_date + timedelta(i), datetime.min.time())\n",
    "    to_time = from_time + timedelta(hours=24)\n",
    "    ds = from_time.strftime(\"%Y%m%d\")\n",
    "    print(f\"正在处理 {from_time} 到 {to_time} 的数据，日期：{ds}\")\n",
    "\n",
    "    ds_click_df = load_user_click_data(ds)\n",
    "    if ds_click_df.empty:\n",
    "        for page_name in [\"/goods\", \"/search/goods\", \"/home\", \"/goods/category\"]:\n",
    "            print(\n",
    "                f\"正在处理 {from_time} 到 {to_time} 的数据，日期：{ds}, page_name:{page_name}\"\n",
    "            )\n",
    "            # 如果本地缓存没有，则重新跑一遍(每一个page都需要跑):\n",
    "            df = get_user_click_data(\n",
    "                from_time=from_time, to_time=to_time, page_name=page_name\n",
    "            )\n",
    "            df[\"ds\"] = ds\n",
    "            df.drop(columns=[\"__source__\", \"__time__\"], inplace=True, errors=\"ignore\")\n",
    "            ds_click_df = pd.concat([ds_click_df, df], ignore_index=True)\n",
    "\n",
    "        # 如果全部获取完了所有页面的数据，则写入本地缓存\n",
    "        ds_click_df.drop_duplicates(\n",
    "            inplace=True,\n",
    "            subset=[\n",
    "                \"uid\",\n",
    "                \"cid\",\n",
    "                \"sid\",\n",
    "                \"pageName\",\n",
    "                \"__tag__:__receive_time__\",\n",
    "                \"__time_ns_part__\",\n",
    "            ],\n",
    "        )\n",
    "        ds_click_df.to_csv(get_csv_location_by_ds(ds=ds), index=False)\n",
    "        print(\n",
    "            f\"已将 {len(ds_click_df)} 条记录保存到 user_click_all_df，总记录数：{len(user_click_all_df)}，日期：{ds}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"从本地缓存中获取到了DS:{ds}的数据,数据条数:{len(ds_click_df)}, pages:{list(ds_click_df['pageName'].unique())}\"\n",
    "        )\n",
    "    user_click_all_df = pd.concat([user_click_all_df, ds_click_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odps_client import get_odps_sql_result_as_df\n",
    "\n",
    "user_view_sql=\"\"\"\n",
    "SELECT  ds\n",
    "        ,cust_id\n",
    "        ,b.variant_id\n",
    "        ,page_name\n",
    "        ,COUNT(1) as sku_view_cnt\n",
    "FROM    summerfarm_tech.dwd_log_mall_di a\n",
    "LEFT JOIN(\n",
    "        SELECT cust_id as mid,min(variant_id) variant_id\n",
    "        FROM summerfarm_ds.temp_mall_new_home_ab_info_di\n",
    "        WHERE ds>='20240910'\n",
    "        GROUP BY cust_id\n",
    ") b ON b.mid=a.cust_id\n",
    "WHERE   a.ds >= '20240910'\n",
    "AND     a.spu_id IS NOT NULL\n",
    "and     a.cust_id is not null\n",
    "AND     a.page_name in ('/goods','/search/goods','/home','/goods/category')\n",
    "group by a.ds\n",
    "        ,a.cust_id\n",
    "        ,b.variant_id\n",
    "        ,a.page_name\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "user_view_data_df=get_odps_sql_result_as_df(sql=user_view_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column '是否点开购物车卡片'\n",
    "user_click_all_df['是否点开购物车卡片'] = '否'\n",
    "user_click_all_df.loc[(user_click_all_df['bid'].str.contains('pid:唤起购买', na=False) | \n",
    "                       (user_click_all_df['pid'] == '唤起购买')), '是否点开购物车卡片'] = '是'\n",
    "\n",
    "user_click_all_df['是否点开商品详情页'] = '否'\n",
    "user_click_all_df.loc[user_click_all_df['bid'].str.contains('pid:goods,sku:', na=False), '是否点开商品详情页'] = '是'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_click_all_clean_df = user_click_all_df[\n",
    "    [\n",
    "        \"uid\",\n",
    "        \"phone\",\n",
    "        \"__tag__:__receive_time__\",\n",
    "        \"是否点开购物车卡片\",\n",
    "        \"是否点开商品详情页\",\n",
    "        \"pageName\",\n",
    "    ]\n",
    "].copy()\n",
    "user_click_all_clean_df.rename(\n",
    "    columns={\"__tag__:__receive_time__\": \"receive_time\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Ensure 'receive_time' is treated as numeric before using 'to_datetime'\n",
    "user_click_all_clean_df[\"receive_time\"] = pd.to_numeric(\n",
    "    user_click_all_clean_df[\"receive_time\"], errors=\"coerce\"\n",
    ")\n",
    "user_click_all_clean_df[\"receive_time\"] = pd.to_datetime(\n",
    "    user_click_all_clean_df[\"receive_time\"], unit=\"s\"\n",
    ")\n",
    "\n",
    "# Create ds column as the date of receive_time in yyyyMMdd format\n",
    "user_click_all_clean_df[\"ds\"] = user_click_all_clean_df[\"receive_time\"].dt.strftime(\n",
    "    \"%Y%m%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql\n",
    "import pandasql.sqldf\n",
    "\n",
    "user_click_analytics_df = pandasql.sqldf(\"\"\"\n",
    "                                       select uid,ds,pageName\n",
    "                                        ,count(case when `是否点开商品详情页` = '是' then 1 end) as 点开商品详情页次数\n",
    "                                        ,count(case when `是否点开购物车卡片` = '是' then 1 end) as 点开购物车卡片次数\n",
    "                                       from user_click_all_clean_df\n",
    "                                       group by uid,ds,pageName\n",
    "                                       \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换连接键为相同类型（字符串）\n",
    "user_view_data_df[\"cust_id\"] = user_view_data_df[\"cust_id\"].fillna(-1)\n",
    "user_view_data_df[\"cust_id\"] = (\n",
    "    user_view_data_df[\"cust_id\"].astype(str).str.replace(\".0\", \"\", regex=False)\n",
    ")\n",
    "user_view_data_df[\"cust_id\"] = user_view_data_df[\"cust_id\"].astype(int)\n",
    "user_view_data_df[\"ds\"] = user_view_data_df[\"ds\"].astype(str)\n",
    "user_view_data_df[\"page_name\"] = user_view_data_df[\"page_name\"].astype(str)\n",
    "\n",
    "user_click_analytics_df = user_click_analytics_df[\n",
    "    user_click_analytics_df[\"uid\"] != \"nan\"\n",
    "]\n",
    "\n",
    "user_click_analytics_df[\"uid\"] = (\n",
    "    user_click_analytics_df[\"uid\"].astype(str).str.replace(\".0\", \"\", regex=False)\n",
    ")\n",
    "user_click_analytics_df[\"uid\"] = user_click_analytics_df[\"uid\"].astype(int)\n",
    "user_click_analytics_df[\"ds\"] = user_click_analytics_df[\"ds\"].astype(str)\n",
    "user_click_analytics_df[\"pageName\"] = user_click_analytics_df[\"pageName\"].astype(str)\n",
    "\n",
    "# 合并 user_view_data_df 和 user_click_analytics_df\n",
    "merged_df = user_view_data_df.merge(\n",
    "    user_click_analytics_df,\n",
    "    left_on=[\"cust_id\", \"ds\", \"page_name\"],\n",
    "    right_on=[\"uid\", \"ds\", \"pageName\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['点开商品详情页次数']=merged_df['点开商品详情页次数'].fillna(0)\n",
    "merged_df['点开购物车卡片次数']=merged_df['点开购物车卡片次数'].fillna(0)\n",
    "merged_df['点开商品详情页次数']=merged_df['点开商品详情页次数'].astype(int)\n",
    "merged_df['点开购物车卡片次数']=merged_df['点开购物车卡片次数'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取用户加入购物车的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_add_cart_data(from_time: datetime, to_time: datetime) -> pd.DataFrame:\n",
    "    query = \"\"\"ap:/shopping/cart/upsert/insert and ai |select \n",
    "json_extract_scalar(json_extract_scalar(ai, '$.data'),'$.sku')sku,\n",
    "json_extract_scalar(json_extract_scalar(ai, '$.data'),'$.quantity')quantity,\n",
    "json_extract(json_extract_scalar(ai, '$.rt'), '$.status') return_status,\n",
    "json_extract_scalar(json_extract_scalar(ai, '$.rt'), '$.msg') return_msg,\n",
    "date_format(__time__, '%Y%m%d') as ds,\n",
    "uid,cid,sid,pageName,url,bid,ap \n",
    "from log limit 10000000\"\"\"\n",
    "\n",
    "    user_add_cart_df = None\n",
    "\n",
    "    user_add_cart_df = get_sls_data_by_query(\n",
    "        from_time=from_time,\n",
    "        to_time=to_time,\n",
    "        query=query,\n",
    "    )\n",
    "    if user_add_cart_df is None:\n",
    "        logging.error(f\"没有获取到用户的加入购物车数据:{from_time}~{to_time}\")\n",
    "        return\n",
    "\n",
    "    return user_add_cart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是获取用户加购物车数据并保存\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from odps_client import write_pandas_df_into_odps\n",
    "\n",
    "\n",
    "def get_add_cart_csv_location_by_ds(ds: str) -> str:\n",
    "    return f\"./data/user_add_cart_all_df_{ds}.csv\"\n",
    "\n",
    "\n",
    "# 如果CSV文件没有数据，则从get_user_click_data获取数据\n",
    "def load_user_add_cart_data(ds_to_run: str) -> pd.DataFrame:\n",
    "    # 尝试从CSV文件读取数据\n",
    "    csv_file_path = get_add_cart_csv_location_by_ds(ds_to_run)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        if not df.empty:\n",
    "            print(f\"从CSV文件加载了{len(df)}条记录\")\n",
    "            return df\n",
    "\n",
    "    print(f\"未找到日期:{ds_to_run}的数据\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "start_date = datetime(2024, 9, 10).date()\n",
    "end_date = datetime.now().date()\n",
    "N = (end_date - start_date).days\n",
    "print(f\"start_date:{start_date}, end_date:{end_date}, N:{N}\")\n",
    "\n",
    "user_add_cart_all_df = pd.DataFrame()\n",
    "\n",
    "for i in range(N):\n",
    "    from_time = datetime.combine(start_date + timedelta(i), datetime.min.time())\n",
    "    to_time = from_time + timedelta(hours=24)\n",
    "    ds = from_time.strftime(\"%Y%m%d\")\n",
    "    print(f\"正在处理 {from_time} 到 {to_time} 的加购物车数据，日期：{ds}\")\n",
    "\n",
    "    ds_add_cart_df = load_user_add_cart_data(ds)\n",
    "    if ds_add_cart_df.empty:\n",
    "        # 如果本地缓存没有，则重新跑一遍\n",
    "        for hour_index in range(0, 24, 6):  # 6小时一批次\n",
    "            print(\n",
    "                f\"正在处理 {from_time} 到 {to_time} 的数据，日期：{ds}, hour_index:{hour_index}\"\n",
    "            )\n",
    "\n",
    "            from_hour = from_time + timedelta(hours=hour_index)\n",
    "            to_hour = from_hour + timedelta(hours=6)  # 6小时一个批次\n",
    "            print(f\"=======>from_hour:{from_hour}, to_hour:{to_hour}\")\n",
    "\n",
    "            df = get_user_add_cart_data(from_time=from_hour, to_time=to_hour)\n",
    "            ds_add_cart_df = pd.concat([ds_add_cart_df, df], ignore_index=True)\n",
    "\n",
    "        # 如果全部获取完了所有页面的数据，则写入本地缓存\n",
    "        ds_add_cart_df.drop_duplicates(\n",
    "            inplace=True,\n",
    "            subset=[\"uid\", \"cid\", \"sid\", \"pageName\", \"__time__\", \"sku\", \"quantity\"],\n",
    "        )\n",
    "        ds_add_cart_df.to_csv(get_add_cart_csv_location_by_ds(ds=ds), index=False)\n",
    "        print(\n",
    "            f\"已将 {len(ds_add_cart_df)} 条记录保存到 user_add_cart_all_df，总记录数：{len(user_add_cart_all_df)}，日期：{ds}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"从本地缓存中获取到了DS:{ds}的数据,数据条数:{len(ds_add_cart_df)}, pages:{list(ds_add_cart_df['pageName'].unique())}\"\n",
    "        )\n",
    "    user_add_cart_all_df = pd.concat(\n",
    "        [user_add_cart_all_df, ds_add_cart_df], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建加购物车汇总数据\n",
    "add_cart_summary_df = pandasql.sqldf(\"\"\"\n",
    "    SELECT uid, ds, pageName AS page_name,\n",
    "           COUNT(DISTINCT sku) AS added_sku_cnt,\n",
    "           SUM(quantity) AS added_quantity,\n",
    "           COUNT(1) AS add_cart_cnt \n",
    "    FROM user_add_cart_all_df \n",
    "    GROUP BY uid, ds, pageName\n",
    "\"\"\")\n",
    "\n",
    "# 转换uid类型为整数\n",
    "add_cart_summary_df['cust_id'] = add_cart_summary_df['uid'].astype(int)\n",
    "add_cart_summary_df['added_quantity'] = add_cart_summary_df['added_quantity'].astype(int)\n",
    "add_cart_summary_df['add_cart_cnt'] = add_cart_summary_df['add_cart_cnt'].astype(int)\n",
    "\n",
    "# 左连接merged_df和add_cart_summary_df\n",
    "merged_all_df = merged_df.merge(\n",
    "    add_cart_summary_df,\n",
    "    how='left',\n",
    "    on=['cust_id', 'ds', 'page_name']\n",
    ")\n",
    "\n",
    "# 显示合并后的数据前10行\n",
    "\n",
    "\n",
    "merged_all_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_static_df=pandasql.sqldf(\"\"\"\n",
    "                  select variant_id\n",
    "                    ,count(distinct cust_id) 总用户数\n",
    "                    ,sum(sku_view_cnt) 用户SKU浏览数\n",
    "                    ,sum(点开商品详情页次数) 点开商品详情页次数\n",
    "                    ,sum(点开购物车卡片次数) 点开购物车卡片次数\n",
    "                    ,sum(add_cart_cnt) 加购成功次数API\n",
    "                    ,sum(added_quantity) SKU总加购件数\n",
    "                    ,round(sum(added_quantity)/count(distinct cust_id),2) 每用户加购SKU件数\n",
    "                    ,round(sum(点开购物车卡片次数) * 100.00/sum(sku_view_cnt),3) as 点击CTR_点开加购卡片\n",
    "                    ,round(sum(点开商品详情页次数) * 100.00/sum(sku_view_cnt),3) as 点击CTR_点到商品详情页\n",
    "                    ,round(sum(点开购物车卡片次数) * 100.00/sum(sku_view_cnt),3) as 点开加购卡片_转化率\n",
    "                    ,round(sum(add_cart_cnt) * 100.00/sum(sku_view_cnt),3) as 加购成功API_转化率\n",
    "                    ,min(ds)||'~'||max(ds) as 日期范围\n",
    "                  from merged_all_df\n",
    "                  where ds between '20240910' and '20240922'\n",
    "                  and page_name in('/search/goods','/goods','/goods/category','/home')\n",
    "                  group by variant_id\n",
    "                  \"\"\")\n",
    "\n",
    "click_static_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_df=pandasql.sqldf(\"\"\"\n",
    "                  select variant_id,page_name\n",
    "                    ,count(distinct cust_id) 总用户数\n",
    "                    ,sum(sku_view_cnt) 用户SKU浏览数\n",
    "                    ,sum(点开商品详情页次数) 点开商品详情页次数\n",
    "                    ,sum(点开购物车卡片次数) 点开购物车卡片次数\n",
    "                    ,sum(add_cart_cnt) 加购成功次数API\n",
    "                    ,sum(added_quantity) SKU总加购件数\n",
    "                    ,round(sum(added_quantity)/count(distinct cust_id),2) 每用户加购SKU件数\n",
    "                    ,round(sum(点开购物车卡片次数) * 100.00/sum(sku_view_cnt),3) as 点击CTR_点开加购卡片\n",
    "                    ,round(sum(点开商品详情页次数) * 100.00/sum(sku_view_cnt),3) as 点击CTR_点到商品详情页\n",
    "                    ,round(sum(点开购物车卡片次数) * 100.00/sum(sku_view_cnt),3) as 点开加购卡片_转化率\n",
    "                    ,round(sum(add_cart_cnt) * 100.00/sum(sku_view_cnt),3) as 加购成功API_转化率\n",
    "                    ,min(ds)||'~'||max(ds) as 日期范围\n",
    "                  from merged_all_df\n",
    "                  where ds>='20240910'\n",
    "                  and page_name in('/search/goods','/goods','/goods/category','/home')\n",
    "                  group by variant_id,page_name\n",
    "                    order by  page_name,variant_id\n",
    "                  \"\"\")\n",
    "\n",
    "page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_df.to_csv(\"分页面的点击转化数据-0910-0922.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_by_yeaterday=(datetime.now()-timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "order_query=f\"\"\"\n",
    "SELECT  variant_id\n",
    "        ,COUNT(DISTINCT order_no) order_cnt\n",
    "        ,COUNT(DISTINCT m_id) 下单用户数\n",
    "        ,SUM(total_price) 总下单GMV\n",
    "        ,round(AVG(total_price),2) 订单均价\n",
    "        ,round(SUM(total_price)/COUNT(DISTINCT m_id),2) 用户平均GMV\n",
    "        ,MIN(order_time)||'~'||MAX(order_time) as time_range\n",
    "FROM    summerfarm_tech.ods_orders_df a\n",
    "LEFT JOIN   (\n",
    "                SELECT  cust_id\n",
    "                        ,MIN(variant_id) AS variant_id\n",
    "                FROM    summerfarm_ds.temp_mall_new_home_ab_info_di\n",
    "                WHERE   ds >= '20240910'\n",
    "                GROUP BY cust_id\n",
    "            ) variant\n",
    "ON      a.m_id = variant.cust_id\n",
    "AND     a.ds = MAX_PT(\"summerfarm_tech.ods_orders_df\")\n",
    "WHERE   a.order_time between '2024-09-10 00:00:00' and '{ds_by_yeaterday} 23:59:59'\n",
    "AND     a.ds = MAX_PT(\"summerfarm_tech.ods_orders_df\")\n",
    "AND     a.status in (2,3,6)\n",
    "GROUP BY variant_id;\n",
    "\"\"\"\n",
    "\n",
    "orders_df=get_odps_sql_result_as_df(order_query)\n",
    "orders_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 左连接orders_df到click_static_df\n",
    "order_merged_result = click_static_df.merge(orders_df, on=\"variant_id\", how=\"left\")\n",
    "\n",
    "# 重命名一些列名以避免冲突\n",
    "order_merged_result = order_merged_result.rename(\n",
    "    columns={\n",
    "        \"总用户数\": \"总浏览用户数\",\n",
    "        \"下单用户数\": \"总下单用户数\",\n",
    "        \"order_cnt\": \"总下单数\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 选择需要的列并重新排序\n",
    "final_result = order_merged_result[\n",
    "    [\n",
    "        \"variant_id\",\n",
    "        \"总浏览用户数\",\n",
    "        \"用户SKU浏览数\",\n",
    "        \"点开商品详情页次数\",\n",
    "        \"点开购物车卡片次数\",\n",
    "        \"加购成功次数API\",\n",
    "        \"SKU总加购件数\",\n",
    "        \"每用户加购SKU件数\",\n",
    "        \"点击CTR_点开加购卡片\",\n",
    "        \"点击CTR_点到商品详情页\",\n",
    "        \"点开加购卡片_转化率\",\n",
    "        \"加购成功API_转化率\",\n",
    "        \"总下单数\",\n",
    "        \"总下单用户数\",\n",
    "        \"总下单gmv\",\n",
    "        \"订单均价\",\n",
    "        \"用户平均gmv\",\n",
    "        \"日期范围\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# 显示结果\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
